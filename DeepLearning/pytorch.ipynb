{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.float32\n",
      "tensor([-0.0335,  3.0265], dtype=torch.float64)\n",
      "tensor([1., 1.], dtype=torch.float64)\n",
      "tensor([0., 0.], dtype=torch.float64)\n",
      "tensor([[0.7512, 0.1036],\n",
      "        [0.0156, 0.0017]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 从列表创建张量\n",
    "a=[1,2,3.]\n",
    "b=torch.tensor(a) \n",
    "print(type(b)) # <class 'torch.Tensor'>\n",
    "print(b.dtype)  # torch.float32\n",
    "\n",
    "# 从numpy数组创建张量\n",
    "a=np.random.normal((2,3))\n",
    "b=torch.tensor(a) \n",
    "print(b) # tensor([2.2654, 2.8027], dtype=torch.float64)\n",
    "\n",
    "# 从张量创建张量\n",
    "c=torch.ones_like(b) #创建与b形状相同的全1张量\n",
    "print(c) # tensor([1., 1.], dtype=torch.float64)\n",
    "c=torch.zeros_like(b) #创建与b形状相同的全0张量\n",
    "print(c) # tensor([0., 0.], dtype=torch.float64)\n",
    "c=torch.rand_like(b) #创建与b形状相同的随机张量\n",
    "# print(c)  # tensor([0.4036, 0.9059], dtype=torch.float64)\n",
    "\n",
    "# 从指定形状创建随机张量(元组和列表)\n",
    "print(torch.rand((2,2)))\n",
    "print(torch.ones([2,2]))\n",
    "print(torch.zeros([2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([1, 2])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 张量的属性\n",
    "a = torch.rand((1, 2))\n",
    "print(a.dtype)  # torch.float32\n",
    "print(a.shape)  # torch.Size([1, 2])\n",
    "print(a.device)  # cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "16\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([0, 2, 4])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 0.]])\n",
      "tensor([[5, 5],\n",
      "        [5, 5]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[0.6468, 0.6278, 0.3850, 0.3910, 0.0236],\n",
      "        [0.1996, 0.9392, 0.0486, 0.6177, 0.5444]])\n",
      "tensor([[0.0056, 0.6860],\n",
      "        [0.0891, 0.5708],\n",
      "        [0.8689, 0.3571],\n",
      "        [0.5334, 0.4560],\n",
      "        [0.7734, 0.2122]])\n",
      "(tensor([[0.6968, 0.8361],\n",
      "        [0.7605, 0.7571]]), tensor([[0.7564, 0.5738]]))\n",
      "tensor([[0.6968],\n",
      "        [0.7605],\n",
      "        [0.7564]])\n",
      "tensor([[0.8361],\n",
      "        [0.7571],\n",
      "        [0.5738]])\n",
      "tensor([[1, 1],\n",
      "        [4, 3]])\n",
      "tensor([[1, 2],\n",
      "        [3, 2]])\n",
      "tensor([[0, 1],\n",
      "        [2, 3]])\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10]])\n",
      "tensor([[1, 0, 0, 4, 0],\n",
      "        [0, 2, 0, 0, 0],\n",
      "        [0, 0, 3, 0, 0]])\n",
      "tensor([[2, 1, 1, 5, 1],\n",
      "        [1, 3, 1, 1, 1],\n",
      "        [1, 1, 4, 1, 1]])\n",
      "(tensor([[0, 1],\n",
      "        [2, 3]]), tensor([[4, 5],\n",
      "        [6, 7]]), tensor([[8, 9]]))\n",
      "(tensor([[0, 1]]), tensor([[2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]]))\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n",
      "tensor([[[[0]],\n",
      "\n",
      "         [[1]]],\n",
      "\n",
      "\n",
      "        [[[2]],\n",
      "\n",
      "         [[3]]],\n",
      "\n",
      "\n",
      "        [[[4]],\n",
      "\n",
      "         [[5]]],\n",
      "\n",
      "\n",
      "        [[[6]],\n",
      "\n",
      "         [[7]]],\n",
      "\n",
      "\n",
      "        [[[8]],\n",
      "\n",
      "         [[9]]]])\n",
      "tensor([[[0.6098, 0.7495],\n",
      "         [0.3663, 0.7162],\n",
      "         [0.9080, 0.9376]],\n",
      "\n",
      "        [[0.2078, 0.8552],\n",
      "         [0.5126, 0.2178],\n",
      "         [0.7506, 0.0387]]])\n",
      "torch.Size([2, 3, 2])\n",
      "torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 张量的操作\n",
    "\n",
    "print(torch.is_tensor(a))  # True\n",
    "print(torch.is_complex(a))  # False\n",
    "print(torch.is_floating_point(a))  # True\n",
    "\n",
    "a=torch.tensor(1.)\n",
    "print(torch.is_nonzero(a))  # True\n",
    "print(torch.is_nonzero(torch.tensor(0.)))  # False\n",
    "print(torch.numel(torch.rand((4,4)))) # 16，返回张量元素个数\n",
    "print(torch.arange(5))  # tensor([0, 1, 2, 3, 4])\n",
    "print(torch.arange(0,5,2)) # tensor([0, 2, 4])\n",
    "print(torch.eye(3)) # 创建单位矩阵\n",
    "print(torch.eye(3,2)) # 创建3x2的单位矩阵\n",
    "print(torch.full((2,2),5)) # 创建2x2的全5张量\n",
    "print(torch.full_like(b,5)) # 创建与b形状相同的全5张量\n",
    "a=torch.rand((2,2))\n",
    "b=torch.rand((2,3))\n",
    "print(torch.cat((a,b),dim=1)) # 按列拼接(第二个维度，下标从0开始)，要求其余维度相同\n",
    "a=torch.rand((2,2))\n",
    "b=torch.rand((3,2))\n",
    "print(torch.cat((a,b),dim=0)) # 按行拼接(第一个维度，下标从0开始)，要求其余维度相同\n",
    "\n",
    "a=torch.rand((3,2))\n",
    "print(torch.chunk(a,chunks=2)) #将张量按行分割为2块\n",
    "b,c=torch.chunk(a,chunks=2,dim=1) #将张量按列分割为2块\n",
    "print(b)\n",
    "print(c)\n",
    "\n",
    "# output[i][j][k]=input[index[i][j][k]][j][k] if dim=0\n",
    "# output[i][j][k]=input[i][index[i][j][k]][k] if dim=1\n",
    "# output[i][j][k]=input[i][j][index[i][j][k]] if dim=2\n",
    "t=torch.tensor([[1,2],[3,4]])\n",
    "print(torch.gather(t,1,torch.tensor([[0,0],[1,0]]))) # tensor([[1, 1],[4, 3]])\n",
    "print(torch.gather(t,0,torch.tensor([[0,0],[1,0]]))) # tensor([[1, 2],[3, 2]])\n",
    "\n",
    "a=torch.arange(0,4)\n",
    "print(torch.reshape(a,(2,2))) # tensor([[0, 1],[2, 3]])\n",
    "print(torch.reshape(a,[-1])) # tensor([0, 1, 2, 3])，-1表示变成一维张量\n",
    "\n",
    "# self[index[i][j][k]][j][k] (+)= src[i][j][k] if dim=0\n",
    "# self[i][index[i][j][k]][k] (+)= src[i][j][k] if dim=1\n",
    "# self[i][j][index[i][j][k]] (+)= src[i][j][k] if dim=2\n",
    "src=torch.arange(1,11).reshape((2,5))\n",
    "print(src)\n",
    "index=torch.tensor([[0,1,2,0]])\n",
    "print(torch.zeros(3,5,dtype=src.dtype).scatter_(0,index,src)) # tensor([[1, 0, 0, 4, 0],[0, 2, 0, 0, 0],[0, 0, 3, 0, 0]])\n",
    "print(torch.ones(3,5,dtype=src.dtype).scatter_add_(0,index,src)) # tensor([[2, 1, 1, 5, 1],[1, 3, 1, 1, 1],[1, 1, 4, 1, 1]])\n",
    "\n",
    "a=torch.arange(10).reshape(5,2)\n",
    "print(torch.split(a,2)) # 按行每2个元素分割一次\n",
    "print(torch.split(a,[1,4])) # 按行分割为1和4两部分\n",
    "\n",
    "print(torch.squeeze(torch.reshape(a,[5,1,2]))) # 去掉维度为1的维度\n",
    "print(torch.squeeze(torch.reshape(a,[5,1,2,1,1]),dim=1))  # 去掉指定维度为1的维度\n",
    "\n",
    "a=torch.rand([3,2])\n",
    "b=torch.rand([3,2])\n",
    "print(torch.stack((a,b),dim=0))\n",
    "print(torch.stack((a,b)).shape) # torch.Size([2, 3, 2])\n",
    "print(torch.stack((a,b),dim=1).shape) # torch.Size([3, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 3])\n",
      "tensor([1, 2, 3, 1, 2, 3])\n",
      "tensor([[1, 2, 3, 1, 2, 3],\n",
      "        [4, 5, 6, 4, 5, 6],\n",
      "        [1, 2, 3, 1, 2, 3],\n",
      "        [4, 5, 6, 4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "(tensor([1, 2, 3]), tensor([4, 5, 6]))\n",
      "(tensor([1, 4]), tensor([2, 5]), tensor([3, 6]))\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([1, 2, 3, 4])\n",
      "torch.Size([2, 1, 3, 4])\n",
      "torch.Size([2, 3, 4, 1])\n",
      "tensor([[0.3390, 1.0000],\n",
      "        [1.0000, 0.0165]])\n",
      "tensor([[0.8615, 0.0865, 0.5069],\n",
      "        [0.4150, 0.2367, 0.5661],\n",
      "        [0.9135, 0.3538, 0.2032]])\n",
      "tensor([[1., 1., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 0., 0.]])\n",
      "tensor([[-3.9117,  3.8843,  1.6605,  3.3893]])\n",
      "tensor([[3, 5],\n",
      "        [5, 0]])\n",
      "tensor([[-1.2535, -0.0592,  1.1878,  1.2247],\n",
      "        [-0.8521, -1.1941,  1.1076,  0.6578],\n",
      "        [-0.0423,  0.4580,  0.1448, -1.4195]])\n",
      "tensor([1, 0, 4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# 张量的操作\n",
    "\n",
    "src=torch.arange(6).reshape(2,3)\n",
    "print(torch.take(src,torch.tensor([0,2,3]))) # 将张量展平后按索引取值\n",
    "\n",
    "a=torch.tensor([1,2,3])\n",
    "print(a.tile((2,))) # tensor([1, 2, 3, 1, 2, 3])，在指定维度上复制张量\n",
    "b=torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(b.tile((2,2)))\n",
    "# tensor([[1, 2, 3, 1, 2, 3],\n",
    "#        [4, 5, 6, 4, 5, 6],\n",
    "#        [1, 2, 3, 1, 2, 3],\n",
    "#        [4, 5, 6, 4, 5, 6]])\n",
    "\n",
    "print(torch.transpose(b,0,1)) # 转置指定维度\n",
    "\n",
    "print(torch.unbind(b,dim=0)) # 按行拆分张量，返回元组\n",
    "print(torch.unbind(b,dim=1)) # 按列拆分张量，返回元组\n",
    "\n",
    "a=torch.rand([2,3,4])\n",
    "print(a.shape) # torch.Size([2, 3, 4])\n",
    "print(torch.unsqueeze(a, dim=0).shape) # torch.Size([1, 2, 3, 4])，在指定维度上增加维度\n",
    "print(torch.unsqueeze(a, dim=1).shape) # torch.Size([2, 1, 3, 4])\n",
    "print(torch.unsqueeze(a, dim=-1).shape) # torch.Size([2, 3, 4, 1])\n",
    "\n",
    "a=torch.rand((2,2))\n",
    "b=torch.ones_like(a)\n",
    "print(torch.where(a<0.5,a,b)) # 小于0.5的元素保留，否则替换为b中对应位置的元素\n",
    "\n",
    "#torch.manual_seed(0) # 设置随机种子\n",
    "\n",
    "a=torch.empty(3,3).uniform_(0,1) # 生成0-1的随机概率\n",
    "print(a)\n",
    "print(torch.bernoulli(a)) # 根据概率生成伯努利分布的随机张量\n",
    "\n",
    "print(torch.normal(2,3,size=(1,4))) # 生成均值为2，标准差为3的正态分布随机张量，大小为1x4\n",
    "\n",
    "print(torch.randint(0,7,(2,2))) # 生成0-9的随机整数张量\n",
    "print(torch.randn(3,4)) # 生成标准正态分布随机张量\n",
    "print(torch.randperm(5)) # 生成0-4的随机排列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
