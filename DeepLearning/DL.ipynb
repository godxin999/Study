{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归模型\n",
    "\n",
    "### 线性回归\n",
    "\n",
    "使用线性回归(linear regression)模型可以预测$f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "通过输入的训练数据来拟合参数 $w$,$b$ 从而最小化预测值 $f_{w,b}(x^{(i)})$ 和实际数据 $y^{(i)}$ 之间的误差。 该度量被称为成本(cost), $J(w,b)$。 在训练过程中会计算所有样本的成本。\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降 \n",
    "\n",
    "梯度下降(gradient descent)被描述为:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "其中参数 $w$, $b$ 同时被更新，即先计算偏导再更新参数。其中 $\\alpha$ 是学习率，用来控制梯度下降时的步长。\n",
    "\n",
    "该公式中梯度被定义为:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "其中 $m$ 是训练样本的数量。\n",
    "\n",
    "梯度下降的缺点是只能求得导数方向，不能求得与最优点的距离，且不能保证全局最优性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多元线性回归\n",
    "\n",
    "多元线性回归模型如下：\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "或者写为向量形式:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "其中 $\\cdot$ 表示向量点积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多元的代价函数 $J(\\mathbf{w},b)$ 为:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "其中:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "其中 $\\mathbf{w}$ 与 $\\mathbf{x}^{(i)}$ 均为标量而非矢量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多元梯度下降的描述如下:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "其中 $n$ 为特征数量，参数 $w_j$，$b$ 同时被更新，即先计算偏导再更新参数。其中 $\\alpha$ 是学习率，用来控制梯度下降时的步长。\n",
    "\n",
    "其中梯度的定义为：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "其中 $m$ 为训练样本的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征缩放\n",
    "\n",
    "通过特征缩放(Feature scaling)可以确保特征在一个相近的范围内，从而使算法更快收敛。\n",
    "\n",
    "可以通过三种方式来进行特征缩放:\n",
    "- 每个特征除以一个选定的值，如特征的绝对值最大值，从而将范围缩放到 $[-1,1]$ 之间。\n",
    "- Mean normalization: 每个特征减去其均值，然后除以最大值与最小值之差，即 $x_i = \\dfrac{x_i - \\mu_i}{max - min} $ \n",
    "- Z-score normalization: 每个特征减去其均值，然后除以标准差，即 $x_i = \\dfrac{x_i - \\mu_i}{\\sigma_i} $\n",
    "\n",
    "#### Z-score normalization\n",
    "\n",
    "在 Z-score normalization 之后，所有特征的平均值为 0，标准差为 1。\n",
    "\n",
    "该标准化的过程如下：\n",
    "\n",
    "$$\n",
    "x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j}\n",
    "$$ \n",
    "其中 $j$ 是特征的索引，$µ_j$ 是该特征的均值 $\\sigma_j$ 是该特征的标准差。\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2 \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率的选择\n",
    "\n",
    "如果在一次迭代中成本函数的值减少的值小于一个很小的值 $\\varepsilon$，如 $0.001$，我们可以认为梯度下降已经收敛。可以根据迭代次数-成本函数的图像来选择学习率，从而获得能够快速收敛的学习率。\n",
    "\n",
    "通常可以使用 $0.001,0.003,0.1,0.3,1\\cdots$ 等值来进行尝试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征工程\n",
    "\n",
    "特征工程即通过转换或者组合特征来创建新的特征。例如，长度和宽度可以组合成面积，从而创建一个新的特征。然后根据原有特征和新特征来进行训练，从而让学习算法做出更好的预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多项式回归\n",
    "\n",
    "多项式回归是线性回归的一种扩展，通过将特征进行多项式转换，从而可以拟合非线性的数据。例如，通过将特征 $x$ 转换为 $x^2$ 或者 $x^3$ 等，从而可以拟合二次或者三次曲线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归\n",
    "\n",
    "逻辑回归(logistic regression)是一种用于解决分类问题的线性模型。逻辑回归的输出是一个概率值，用来表示样本属于某个类别的概率。\n",
    "\n",
    "逻辑回归的模型如下： \n",
    "\n",
    "$$\n",
    "f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) \\tag{1}\n",
    "$$\n",
    "对于逻辑回归模型，$z=\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b$，故 $g(z)$ 为激活函数(sigmoid function)，其将所有的输入值映射到 $[0,1]$ 之间:\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1+e^{-z}} \\tag{2}\n",
    "$$\n",
    "\n",
    "- 如果 $f_{\\mathbf{w},b}(x) >= 0.5$，即 $z\\ge 0$，则预测 $y=1$\n",
    "- 如果 $f_{\\mathbf{w},b}(x) < 0.5$，即 $z\\lt 0$，则预测 $y=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策边界\n",
    "\n",
    "根据拟合出的参数 $\\mathbf{w}$ 和 $b$，可以绘制出决策边界(decision boundary)，决策边界的坐标轴为特征的维度，通过绘制决策边界，可以直观的看出模型的分类效果。决策边界可以是线性的，也可以是非线性的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归的代价函数\n",
    "\n",
    "逻辑回归的代价函数 $J(\\mathbf{w},b)$ 为:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ 为损失函数，用来衡量预测值 $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ 与实际值 $y^{(i)}$ 之间的差异。逻辑回归的损失函数为:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) &= -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\newline &- \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "对于该损失函数，当 $y^{(i)}=0$ 时，第一项为 $0$，第二项为 $-\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$，当 $f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\to 1$ 时，损失函数趋近于 $0$；当 $y^{(i)}=1$ 时，第一项为 $-\\log \\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$，第二项为 $0$，当 $f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\to 0$ 时，损失函数趋近于 $0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归的梯度下降\n",
    "\n",
    "逻辑回归的梯度下降的描述如下:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "其中 $n$ 为特征数量，参数 $w_j$，$b$ 同时被更新，即先计算偏导再更新参数。其中 $\\alpha$ 是学习率，用来控制梯度下降时的步长。\n",
    "\n",
    "其中梯度的定义为：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}\n",
    "\\end{align}\n",
    "$$\n",
    "其中 $m$ 为训练样本的数量。\n",
    "\n",
    "虽然形式上与线性回归的梯度下降相同，但逻辑回归的模型是 $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)$，线性回归的模型是 $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "\n",
    "在拟合模型时，有时会出现过拟合(overfitting)的情况，即模型在训练集上表现很好，但在测试集上表现很差。过拟合可以通过增加训练样本数量，减少特征数量，或者正则化(regularization)来解决。\n",
    "\n",
    "正则化的原理就是在代价函数中增加惩罚项来减小参数的值，从而防止过拟合。通常的正则化项为 $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$\n",
    "    \n",
    "对于线性回归和逻辑回归，正则化后的梯度为：\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{1} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{2} \n",
    "\\end{align*}$$\n",
    "\n",
    "其中，$\\lambda$ 为正则化参数，用来控制正则化的强度。$m$ 为训练样本的数量。\n",
    "\n",
    "根据梯度下降的步骤：\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "我们可以提取出：\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_j &= (1 - \\alpha \\frac{\\lambda}{m})w_j -  \\alpha \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{3} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "根据这个式子，我们可以观察到正则化参数 $\\lambda$ 的作用是每次迭代减小 $w_j$ 的值，从而防止过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "\n",
    "### 激活函数\n",
    "\n",
    "激活函数(activation function)是神经网络中的一个重要组成部分，它决定了神经元的输出。常用的激活函数有 sigmoid 函数、tanh 函数、ReLU 函数等。\n",
    "\n",
    "- sigmoid 函数: $g(z) = \\frac{1}{1+e^{-z}}$，用于二分类问题\n",
    "- ReLU 函数: $g(z) = \\max(0,z)$，用于输出值为非负数的情况\n",
    "- linear 函数: $g(z) = z$，用于输出值为任意实数的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "Softmax 函数是一种用于多分类问题的激活函数，它将神经元的输出值转换为概率值。Softmax 函数的定义如下：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{softmax}(\\mathbf{z})_i &= \\frac{e^{z_i}}{\\sum_{j=0}^{n-1} e^{z_j}} \\tag{1} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中 $z$ 为神经元的输出值，$n$ 为特征的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax 的损失函数为交叉熵损失函数(cross-entropy loss function)。交叉熵损失函数的定义如下：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\mathbf{z}) &= -\\frac{1}{m} \\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} y^{(i)}_j \\log \\left( \\text{softmax}(\\mathbf{z}^{(i)})_j \\right) \\tag{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中 $y^{(i)}_j$ 为样本 $i$ 是否属于类别 $j$ 的布尔值。\n",
    "\n",
    "还有稀疏交叉熵损失函数(sparse cross-entropy loss function)，其定义如下：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\mathbf{z}) &= -\\frac{1}{m} \\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} 1\\{y^{(i)}== j\\} \\log \\left( \\text{softmax}(\\mathbf{z}^{(i)})_j \\right) \\tag{3}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中 $y^{(i)}$ 为样本 $i$ 的实际类型取值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 1s 5ms/step - loss: 1.7868 - val_loss: 1.3745\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.2093 - val_loss: 1.1038\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.0076 - val_loss: 0.9399\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8522 - val_loss: 0.7815\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7097 - val_loss: 0.6573\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6088 - val_loss: 0.5797\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5482 - val_loss: 0.5317\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5080 - val_loss: 0.4961\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.4771 - val_loss: 0.4675\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.4515 - val_loss: 0.4432\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.4290 - val_loss: 0.4221\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4090 - val_loss: 0.4026\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.3907 - val_loss: 0.3848\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.3685\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.3530\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.3433 - val_loss: 0.3392\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.3298 - val_loss: 0.3255\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3131\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3047 - val_loss: 0.3010\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2932 - val_loss: 0.2901\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2828 - val_loss: 0.2800\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2726 - val_loss: 0.2695\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.2631 - val_loss: 0.2600\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.2540 - val_loss: 0.2512\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2444 - val_loss: 0.2407\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2300 - val_loss: 0.2231\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2134 - val_loss: 0.2080\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1990 - val_loss: 0.1904\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1760 - val_loss: 0.1629\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1545 - val_loss: 0.1462\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1440 - val_loss: 0.1391\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1384 - val_loss: 0.1344\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1340 - val_loss: 0.1308\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1304 - val_loss: 0.1274\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1273 - val_loss: 0.1240\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1245 - val_loss: 0.1214\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1221 - val_loss: 0.1187\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1194 - val_loss: 0.1165\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1171 - val_loss: 0.1141\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1153 - val_loss: 0.1120\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1135 - val_loss: 0.1107\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1115 - val_loss: 0.1080\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.1096 - val_loss: 0.1061\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1080 - val_loss: 0.1042\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1062 - val_loss: 0.1028\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1048 - val_loss: 0.1015\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1032 - val_loss: 0.0997\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1020 - val_loss: 0.0988\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1006 - val_loss: 0.0966\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0994 - val_loss: 0.0967\n",
      "WARNING:tensorflow:5 out of the last 23 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D30AF31438> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[-1.35231146 -0.47697014]\n",
      " [-4.50328585  1.8617357 ]]\n",
      "[[0.02864943 0.9022494  0.06154283 0.00755833]\n",
      " [0.9908262  0.0056769  0.00165204 0.00184484]]\n"
     ]
    }
   ],
   "source": [
    "# softmax regression\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "x_train, y_train = make_blobs(\n",
    "    n_samples=2000, centers=centers, cluster_std=1.0, random_state=30)\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.2,random_state=42)\n",
    "\n",
    "x_test, y_test = make_blobs(\n",
    "    n_samples=2, centers=centers, cluster_std=1.0, random_state=42)\n",
    "\n",
    "\n",
    "model=Sequential([\n",
    "    #kernel_regularizer表示权重的正则化\n",
    "    Dense(units=16,activation='relu',kernel_regularizer=l2(0.01)),\n",
    "    Dense(units=8,activation='relu',kernel_regularizer=l2(0.01)),\n",
    "    Dense(units=4,activation='linear')\n",
    "])\n",
    "\n",
    "# from_logits=True表示未经过softmax层，计算更加稳定\n",
    "# 采用Adam算法进行优化\n",
    "model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))\n",
    "# validation_data是交叉验证数据集\n",
    "model.fit(x_train,y_train,epochs=50,validation_data=(x_val,y_val))\n",
    "\n",
    "p = model.predict(x_test)\n",
    "# 进行softmax处理\n",
    "sm = tf.nn.softmax(p).numpy()\n",
    "\n",
    "print(x_test)\n",
    "print(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 倾斜数据集的误差指标\n",
    "\n",
    "对于倾斜的数据集，即正负样本的数量差异很大的数据集，我们不能使用准确率(accuracy)作为误差指标。因为即使模型预测所有样本都为负样本，也可以获得很高的准确率。例如：某种罕见疾病的检测，正样本占总样本的 $1\\%$，即使模型预测所有样本都为负样本，也可以获得 $99\\%$ 的准确率。\n",
    "\n",
    "对于倾斜的数据集，我们可以使用精确率(precision)和召回率(recall)作为误差指标。精确率和召回率的定义如下：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{precision} &= \\frac{TP}{TP+FP} \\tag{1} \\\\\n",
    "\\text{recall} &= \\frac{TP}{TP+FN} \\tag{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中 $TP$ 为真正例(true positive)，即模型预测为正样本的样本中实际为正样本的数量，$FP$ 为假正例(false positive)，即模型预测为正样本的样本中实际为负样本的数量，$FN$ 为假负例(false negative)，即模型预测为负样本的样本中实际为正样本的数量。\n",
    "\n",
    "提升精确率可以减少错报，提升召回率可以减少漏报。\n",
    "\n",
    "精确率和召回率是一对矛盾的指标，即提高精确率会降低召回率，提高召回率会降低精确率。我们可以使用 F1 值来综合考虑精确率和召回率，F1 值的定义如下：\n",
    "\n",
    "$$\n",
    "\\text{F1} = \\frac{2}{{\\frac{1}{P}} + \\frac{1}{R}} \\tag{3}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树\n",
    "\n",
    "决策树是一种用于分类和回归的监督学习算法。决策树的目标是通过对特征的提问来预测目标变量的值。决策树的每个非叶节点代表一个特征，每个分支代表一个特征的取值，每个叶子节点代表目标变量的值。\n",
    "\n",
    "决策树根据纯度(purity)来选择特征，我们可以根据熵(entropy)来衡量纯度。熵的定义如下：\n",
    "\n",
    "$$\n",
    "\\text{Entropy} = -\\sum_{i=0}^{n-1} p_i \\log_2 p_i \\tag{1}\n",
    "$$\n",
    "\n",
    "其中 $p_i$ 为类别 $i$ 占当前节点样本数的比例。\n",
    "\n",
    "进一步，我们根据熵的值可以计算出信息增益(information gain)，信息增益的定义如下：\n",
    "\n",
    "$$\n",
    "\\text{Information Gain} = \\text{Entropy(parent)} - \\sum_{i=0}^{n-1} \\frac{N_i}{N} \\text{Entropy(child}_i) \\tag{2}\n",
    "$$\n",
    "\n",
    "其中 $N_i$ 为子节点 $i$ 的样本数量，$N$ 为父节点的样本数量。\n",
    "\n",
    "我们选择信息增益最大的特征进行划分，从而创建树的左右分支，持续划分直到\n",
    "\n",
    "- 一个叶节点纯度为100%，即全为同一种类\n",
    "- 当拆分节点时超过了设定的最大深度\n",
    "- 信息增益小于阈值\n",
    "- 节点中样本数小于阈值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding\n",
    "\n",
    "One-Hot encoding 是一种用于处理分类特征的编码方式。对于分类特征，我们可以使用 One Hot encoding 将其转换为二进制编码。例如，对于特征 $x$，其取值为 $a,b,c$，我们可以将其转换为三个特征 $x_a,x_b,x_c$，其中 $x_a=1$ 表示 $x=a$，$x_b=1$ 表示 $x=b$，$x_c=1$ 表示 $x=c$。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连续的有价值特征\n",
    "\n",
    "对于连续的有价值特征，我们可以使用决策树来进行划分。例如，对于特征 $x$，我们可以选择一个阈值 $t$，将样本分为 $x<t$ 和 $x\\ge t$ 两部分。我们可以选择信息增益最大的阈值来进行划分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归树\n",
    "\n",
    "除了用于分类，决策树还可以用于回归。对于回归树，我们可以使用方差来衡量纯度，然后使用方差来计算信息增益，计算的方式如下：\n",
    "\n",
    "$$\n",
    "\\text{Variance} = \\frac{1}{N} \\sum_{i=0}^{N-1} (y_i - \\bar{y})^2 \\tag{1}\n",
    "$$\n",
    "\n",
    "其中 $y_i$ 为样本 $i$ 的目标变量的值，$\\bar{y}$ 为所有样本的目标变量的均值。\n",
    "\n",
    "$$\n",
    "\\text{Information Gain} = \\text{Variance(parent)} - \\sum_{i=0}^{n-1} \\frac{N_i}{N} \\text{Variance(child}_i) \\tag{2}\n",
    "$$\n",
    "\n",
    "对于回归树，我们选择信息增益最大的特征进行划分，划分停止的条件与分类树相同，此时预测的值为叶节点中样本的目标变量的均值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林\n",
    "\n",
    "随机森林是一种集成学习(ensemble learning)算法，它通过训练多个决策树，然后将它们的预测结果进行平均来提高预测的准确率。随机森林的训练过程如下：\n",
    "\n",
    "- 从训练集中有放回的抽取 $m$ 个样本\n",
    "- 从 $n$ 个特征中随机选择 $k$ 个特征，一般取 $k=\\sqrt{n}$，并保证决策树只从该 $k$ 个特征中选择最佳特征进行划分\n",
    "- 训练决策树\n",
    "- 重复上述步骤 $N$ 次\n",
    "- 对于分类问题，将 $N$ 个决策树的预测结果进行投票，对于回归问题，将 $N$ 个决策树的预测结果进行平均\n",
    "- 得到最终的预测结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "XGBoost 是一种梯度提升树(gradient boosting tree)算法，其训练过程和随机森林类似，但是其会在训练决策树后对训练集进行预测，然后提高错误样本的权重(抽取的概率)，从而训练下一个决策树。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:05:25] WARNING: C:\\Users\\dev-admin\\croot\\xgboost-split_1675120659361\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1]\n",
      "[0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "weights=np.random.randint(1,10,size=100)\n",
    "whiskers=np.random.randint(0,2,size=100)\n",
    "faces=np.random.randint(0,3,size=100)\n",
    "hair=np.random.randint(0,5,size=100)\n",
    "targets=np.random.randint(0,2,size=100)\n",
    "\n",
    "data=np.column_stack([weights,whiskers,faces,hair])\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(data,targets,test_size=0.2,random_state=42)\n",
    "\n",
    "# max_depth表示树的最大深度\n",
    "# learning_rate表示学习率\n",
    "# objective表示目标函数，这里是二分类，返回经过sigmoid函数处理的概率\n",
    "model = XGBClassifier(max_depth=3,learning_rate=0.1,objective='binary:logistic')\n",
    "\n",
    "model.fit(x_train,y_train)\n",
    "# predict返回的是类别，predict_proba返回的是概率\n",
    "p = model.predict(x_test)\n",
    "\n",
    "print(y_test)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树的适用场景\n",
    "\n",
    "决策树适用于结构化数据，例如表格，但是其不适用于非结构化数据，例如图像和音频。决策树的优点是易于理解和解释且速度快。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "\n",
    "K-means 是一种无监督学习算法，其目标是将 $n$ 个样本划分为 $k$ 个簇，使得每个样本都属于离其最近的簇。K-means 的训练过程如下：\n",
    "\n",
    "- 随机初始化 $k$ 个簇的中心\n",
    "- 对于每个样本，计算其到 $k$ 个簇中心的距离，将其划分到离其最近的簇\n",
    "- 更新每个簇的中心为其样本的均值\n",
    "- 重复上述步骤直到簇的中心不再变化\n",
    "\n",
    "$c^{(i)}$ 为样本 $i$ 的簇的索引，$\\mu_j$ 为簇 $j$ 的中心，$\\mu_{c^{(i)}}$ 为样本 $i$ 的簇的中心，其中 $k$ 为簇的数量。故 K-means 的代价函数为:\n",
    "\n",
    "$$\n",
    "J(c^{(1)},\\cdots,c^{(m)},\\mu_1,\\cdots,\\mu_k    \n",
    ") = \\frac{1}{m} \\sum_{i=0}^{m-1} ||x^{(i)} - \\mu_{c^{(i)}}||^2 \\tag{1}\n",
    "$$\n",
    "\n",
    "其中 $m$ 为样本的数量。\n",
    "\n",
    "在更新簇的中心时，我们可以使用均值来更新，即:\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^{(i)} \\tag{2}\n",
    "$$\n",
    "\n",
    "其中 $C_k$ 为簇 $k$ 中的样本的索引，$|C_k|$ 为簇 $k$ 中的样本的数量。\n",
    "\n",
    "在初始化 $k$ 个簇的中心时，可以选择样本中的 $k$ 个样本作为初始的簇的中心，多次选择并计算代价函数，选择代价函数最小的一次作为初始的簇的中心。\n",
    "\n",
    "$k$ 的数量应该根据进一步的需求进行选取。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
